import streamlit as st
import time
from deep_translator import GoogleTranslator
from bert_engine.embedder import BertEmbedder
from bert_engine.searcher import SemanticSearcher
from bert_engine.database import MySQLDocumentDB

st.title("AI å‘é‡æ£€ç´¢ç³»ç»Ÿ")

if "all_documents" not in st.session_state:
    st.session_state.all_documents = []
if "current_results" not in st.session_state:
    st.session_state.current_results = []
if "search_done" not in st.session_state:
    st.session_state.search_done = False
if "embedder" not in st.session_state:
    st.session_state.embedder = BertEmbedder()
if "elapsed_time" not in st.session_state:
    st.session_state.elapsed_time = 0.0

query = st.text_input("è¯·è¾“å…¥æ£€ç´¢å†…å®¹ï¼ˆä¸­æ–‡å°†è‡ªåŠ¨ç¿»è¯‘ä¸ºè‹±æ–‡ï¼‰")
field = st.selectbox("é€‰æ‹©è®¡ç®—ç›¸ä¼¼åº¦æ ‡å‡†", ["title", "abstract", "authors", "category", "title+abstract"], index=4)
sort_by = st.selectbox("æ’åºæ–¹å¼", ["relevance", "date"])
page_size = st.selectbox("æ¯é¡µå±•ç¤ºç»“æœæ•°é‡", [5, 10, 20, 50])
similarity_threshold = st.slider("ç›¸å…³æ€§ä¸‹é™å€¼ï¼ˆç›¸ä¼¼åº¦ï¼‰", 0.2, 0.4, 0.25, step=0.01)
current_page = st.number_input("é¡µç ", min_value=1, value=1, step=1)

col1, col2 = st.columns([1, 1])
search_triggered = col1.button("å¼€å§‹æ£€ç´¢")
refine_triggered = col2.button("åœ¨ç»“æœä¸­æ£€ç´¢")

if (search_triggered or refine_triggered) and query.strip():
    start_time = time.time()
    translated_query = GoogleTranslator(source='auto', target='en').translate(query)
    if search_triggered:

#### è¿™è¾¹å†™ä¸€ä¸‹è‡ªå·±sql workbenchçš„å¯†ç 
        db = MySQLDocumentDB(password='123456')
        st.session_state.all_documents = db.fetch_all_documents()
        source_docs = st.session_state.all_documents
    else:
        source_docs = st.session_state.current_results

    documents = [{
        "id": doc["id"] if refine_triggered else doc["paper_id"],
        "title": doc["title"],
        "abstract": doc["abstract"],
        "authors": doc["authors"],
        "category": doc["category"],
        "published": doc["published"]
    } for doc in source_docs]

    searcher = SemanticSearcher(st.session_state.embedder, documents, field=field)
    all_results = searcher.search(translated_query, top_k=1000, sort_by=sort_by)
    filtered_results = [(pid, score) for pid, score in all_results if score >= similarity_threshold]

    result_ids_sorted = [pid for pid, _ in filtered_results]
    st.session_state.current_results = [next(doc for doc in documents if doc["id"] == pid) for pid in result_ids_sorted]
    st.session_state.filtered_result_scores = {pid: score for pid, score in filtered_results}
    st.session_state.search_done = True
    st.session_state.elapsed_time = time.time() - start_time

if st.session_state.search_done and st.session_state.current_results:
    total_results = len(st.session_state.current_results)
    total_pages = (total_results + page_size - 1) // page_size
    if current_page > total_pages:
        st.warning(f"é¡µç è¶…è¿‡æœ€å¤§é¡µæ•°ï¼ˆå…± {total_pages} é¡µï¼‰ï¼Œè¯·é‡æ–°é€‰æ‹©ã€‚")
    else:
        st.markdown(f"â± æœ¬æ¬¡æ£€ç´¢è€—æ—¶: **{st.session_state.elapsed_time:.2f} ç§’**")
        st.markdown(f"ğŸ” å…±æ‰¾åˆ° {total_results} æ¡ç›¸å…³ç»“æœï¼Œå½“å‰ä¸ºç¬¬ {current_page} é¡µï¼Œæ¯é¡µæ˜¾ç¤º {page_size} æ¡ï¼š")

        start_idx = (current_page - 1) * page_size
        end_idx = start_idx + page_size
        page_results = st.session_state.current_results[start_idx:end_idx]

#### è¿™è¾¹å†™ä¸€ä¸‹è‡ªå·±sql workbenchçš„å¯†ç 
        db = MySQLDocumentDB(password='123456')
        for doc in page_results:
            score = st.session_state.filtered_result_scores.get(doc["id"], 0)
            st.markdown("---")
            st.markdown(f"ğŸ“Œ **{doc['title']}**")
            st.markdown(f"ğŸ“… å‘å¸ƒæ—¶é—´: {doc['published']}")
            st.markdown(f"ğŸ§  ç›¸ä¼¼åº¦: {score:.2f}")
            with st.expander("ğŸ“– æŸ¥çœ‹æ‘˜è¦å…¨æ–‡"):
                st.markdown(f"{doc['abstract']}")
                
            ## gpt summary 
            with st.expander("ğŸ“˜ GPTæ‘˜è¦æŸ¥çœ‹"):
                try:
                    summary = db.fetch_summary_by_id(doc["id"])
                    st.markdown(summary if summary else "âš ï¸ å½“å‰è®ºæ–‡æš‚æ— æ‘˜è¦æ•°æ®ã€‚")
                except Exception as e:
                    st.markdown(f"âŒ è·å–æ‘˜è¦å¤±è´¥: {e}")